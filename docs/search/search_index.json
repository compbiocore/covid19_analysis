{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Running Workflow on OSCAR The following documentation details on how to run the Covid19 analysis pipeline specifically on Brown's Oscar cluster. Directory Structure 0_data: is an empty directory in which to download sequneces and metadata from GISAID for analyses. 1_scripts: contains shell scripts to run the pipeline as reflected in /covid19_analysis/1_scripts the singularity image can be pulled directly to oscar or your local machine using singularity pull covid19.sif docker://ericsalomaki/covid_new_pango:05092023 from the 1_scripts directory. 2_metadata: contains the Dockerfile that was used to create the container for running the pipeline, a GFF file, QC rules file, and the reference fasta file and genbank file. 3_results will be created while the pipeline is running and results will be written to /covid19_analysis/3_results/${YYYYMMDD} Running Pipeline via Oscar Slurm Batch Submission To run the covid pipeline, navigate to /PATH/TO/CLONED/REPO/covid19_analysis/1_scripts/ and run: sbatch run_slurm.sh /ABSOLUTE/PATH/TO/SEQUENCE/DATA/covid_sequences.fasta Results will be produced in /covid19_analysis/3_results/${YYYYMMDD} A run with ~20,000 input sequences takes roughly 30 minutes to complete the primary pangolin analyses and produce figures on Oscar with 24 threads and 128G RAM allocated, however the IQ-tree analysis will run for several days. If incomplete, IQ-tree uses checkpoints and therefore the analysis can be continued beyond the allocated time, if necessary. Running Pipeline via Oscar Interactive Session To run thie pipeline in an interact session, first enter a screen screen -S JOBNAME and then initiate an interact session with enough resources ( interact -t 24:00:00 -n 24 -m 128G ) Navigate to the 1_scripts directory: cd /PATH/TO/CLONED/REPO/covid19_analysis/1_scripts Enter the singularity container and mount the parent directory: singularity exec -B /ABSOLUTE/PATH/TO/CLONED/REPO/covid19_analysis/ /PATH/TO/CLONED/REPO/covid19_analysis/1_scripts/covid19.sif bash Once inside the container, run: bash run.sh /ABSOLUTE/PATH/TO/SEQUENCE/DATA/covid_sequences.fasta To leave the screen use ctl + a + d and to return use screen -r JOBNAME Results will be produced in /PATH/TO/CLONED/REPO/covid19_analysis/3_results/${YYYYMMDD} Example Usage for Oscar sbatch /PATH/TO/CLONED/REPO/covid19_analysis/1_scripts/run_slurm.sh /PATH/TO/CLONED/REPO/covid19_analysis/0_data/sequenceData.fasta","title":"Running Workflow on OSCAR"},{"location":"#running-workflow-on-oscar","text":"The following documentation details on how to run the Covid19 analysis pipeline specifically on Brown's Oscar cluster.","title":"Running Workflow on OSCAR"},{"location":"#directory-structure","text":"0_data: is an empty directory in which to download sequneces and metadata from GISAID for analyses. 1_scripts: contains shell scripts to run the pipeline as reflected in /covid19_analysis/1_scripts the singularity image can be pulled directly to oscar or your local machine using singularity pull covid19.sif docker://ericsalomaki/covid_new_pango:05092023 from the 1_scripts directory. 2_metadata: contains the Dockerfile that was used to create the container for running the pipeline, a GFF file, QC rules file, and the reference fasta file and genbank file. 3_results will be created while the pipeline is running and results will be written to /covid19_analysis/3_results/${YYYYMMDD}","title":"Directory Structure"},{"location":"#running-pipeline-via-oscar-slurm-batch-submission","text":"To run the covid pipeline, navigate to /PATH/TO/CLONED/REPO/covid19_analysis/1_scripts/ and run: sbatch run_slurm.sh /ABSOLUTE/PATH/TO/SEQUENCE/DATA/covid_sequences.fasta Results will be produced in /covid19_analysis/3_results/${YYYYMMDD} A run with ~20,000 input sequences takes roughly 30 minutes to complete the primary pangolin analyses and produce figures on Oscar with 24 threads and 128G RAM allocated, however the IQ-tree analysis will run for several days. If incomplete, IQ-tree uses checkpoints and therefore the analysis can be continued beyond the allocated time, if necessary.","title":"Running Pipeline via Oscar Slurm Batch Submission"},{"location":"#running-pipeline-via-oscar-interactive-session","text":"To run thie pipeline in an interact session, first enter a screen screen -S JOBNAME and then initiate an interact session with enough resources ( interact -t 24:00:00 -n 24 -m 128G ) Navigate to the 1_scripts directory: cd /PATH/TO/CLONED/REPO/covid19_analysis/1_scripts Enter the singularity container and mount the parent directory: singularity exec -B /ABSOLUTE/PATH/TO/CLONED/REPO/covid19_analysis/ /PATH/TO/CLONED/REPO/covid19_analysis/1_scripts/covid19.sif bash Once inside the container, run: bash run.sh /ABSOLUTE/PATH/TO/SEQUENCE/DATA/covid_sequences.fasta To leave the screen use ctl + a + d and to return use screen -r JOBNAME Results will be produced in /PATH/TO/CLONED/REPO/covid19_analysis/3_results/${YYYYMMDD}","title":"Running Pipeline via Oscar Interactive Session"},{"location":"#example-usage-for-oscar","text":"sbatch /PATH/TO/CLONED/REPO/covid19_analysis/1_scripts/run_slurm.sh /PATH/TO/CLONED/REPO/covid19_analysis/0_data/sequenceData.fasta","title":"Example Usage for Oscar"},{"location":"workflow/","text":"Running Workflow via Nextflow The following documentation details on how to run the Covid19 analysis pipeline using Nextflow on any computing environment. Installation 1. Check out Github repo First, check out the Github repo: git clone https://github.com/compbiocore/covid19_analysis.git 2. Install Nextflow and Singularity Option A: On Any Computing Environment If you do not have Singularity already; you can install it by referring to the Singularity installation guide here. If you do not have Nextflow already; you can install it by referring to the Nextflow installation guide here. After installing Singularity, ensure that in your Nextflow configuration file, you have enabled Singularity in Nextflow. You can refer to the Singularity configuration guide here; or in another words, add the following block in the nextflow.config file that Nextflow is sourcing: ... singularity { enabled = true } Option B: On Brown OSCAR Computing Environment If you are on Brown OSCAR computing environment, you can simply install Nextflow and Singularity computing environment by following the set up instructions here . And then to initialize the Nextflow environment, simply type in: nextflow_start Running the Nextflow Workflow Once you have finished installing (or already have the requisites satisfied), you can run the Nextflow pipeline with the following command: cd $PROJECT_REPO nextflow run $PROJECT_REPO/workflows/covid19.nf \\ --output_dir $OUTPUT_DIR --username $GISAID_USER --password='$GISAID_PASSWORD' \\ --project_github $PROJECT_REPO Output Directory Below is a brief walk-through and explaination of all the workflow workproducts: Output 1: GISAID Sequence Files and Metadata In $OUTPUT_DIR/gisaid : - gisaid.fasta , the sequence containing for all sequences downloaded from GISAID given a certain geolocation (e.g., USA/Rhode Island). - gisaid.csv , the GISAID metadata file for all the sequences given the certain geolocation - sra_run.txt , all of the SRA id's linked to the GISAID sequences in this workflow. Output 2: Analysis Files","title":"Running Workflow via Nextflow"},{"location":"workflow/#running-workflow-via-nextflow","text":"The following documentation details on how to run the Covid19 analysis pipeline using Nextflow on any computing environment.","title":"Running Workflow via Nextflow"},{"location":"workflow/#installation","text":"","title":"Installation"},{"location":"workflow/#1-check-out-github-repo","text":"First, check out the Github repo: git clone https://github.com/compbiocore/covid19_analysis.git","title":"1. Check out Github repo"},{"location":"workflow/#2-install-nextflow-and-singularity","text":"","title":"2. Install Nextflow and Singularity"},{"location":"workflow/#option-a-on-any-computing-environment","text":"If you do not have Singularity already; you can install it by referring to the Singularity installation guide here. If you do not have Nextflow already; you can install it by referring to the Nextflow installation guide here. After installing Singularity, ensure that in your Nextflow configuration file, you have enabled Singularity in Nextflow. You can refer to the Singularity configuration guide here; or in another words, add the following block in the nextflow.config file that Nextflow is sourcing: ... singularity { enabled = true }","title":"Option A: On Any Computing Environment"},{"location":"workflow/#option-b-on-brown-oscar-computing-environment","text":"If you are on Brown OSCAR computing environment, you can simply install Nextflow and Singularity computing environment by following the set up instructions here . And then to initialize the Nextflow environment, simply type in: nextflow_start","title":"Option B: On Brown OSCAR Computing Environment"},{"location":"workflow/#running-the-nextflow-workflow","text":"Once you have finished installing (or already have the requisites satisfied), you can run the Nextflow pipeline with the following command: cd $PROJECT_REPO nextflow run $PROJECT_REPO/workflows/covid19.nf \\ --output_dir $OUTPUT_DIR --username $GISAID_USER --password='$GISAID_PASSWORD' \\ --project_github $PROJECT_REPO","title":"Running the Nextflow Workflow"},{"location":"workflow/#output-directory","text":"Below is a brief walk-through and explaination of all the workflow workproducts:","title":"Output Directory"},{"location":"workflow/#output-1-gisaid-sequence-files-and-metadata","text":"In $OUTPUT_DIR/gisaid : - gisaid.fasta , the sequence containing for all sequences downloaded from GISAID given a certain geolocation (e.g., USA/Rhode Island). - gisaid.csv , the GISAID metadata file for all the sequences given the certain geolocation - sra_run.txt , all of the SRA id's linked to the GISAID sequences in this workflow.","title":"Output 1: GISAID Sequence Files and Metadata"},{"location":"workflow/#output-2-analysis-files","text":"","title":"Output 2: Analysis Files"}]}